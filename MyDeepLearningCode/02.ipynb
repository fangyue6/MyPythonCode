{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://zhuanlan.zhihu.com/p/39599265\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    w_x = X.shape[1]\n",
    "    n_h = 4 # size of hidden layer\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y,w_x)\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y,w_x):\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, w_x))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, w_x)) \n",
    "   \n",
    "    assert (W1.shape == (n_h, n_x))    \n",
    "    assert (b1.shape == (n_h, w_x))    \n",
    "    assert (W2.shape == (n_y, n_h))    \n",
    "    assert (b2.shape == (n_y, w_x))\n",
    "    parameters = {\"W1\": W1, \n",
    "                  \"b1\": b1,                 \n",
    "                  \"W2\": W2,                  \n",
    "                  \"b2\": b2}   \n",
    "                   \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, Z1) + b2\n",
    "    A2 = sigmoid(Z2)    \n",
    "    print(\"A2.shape=\",A2.shape,\" X.shape=\",X.shape)\n",
    "    #assert(A2.shape == (1, X.shape[1]))\n",
    "    assert(A2.shape == X.shape)\n",
    "    cache = {\"Z1\": Z1,                   \n",
    "             \"A1\": A1,                   \n",
    "             \"Z2\": Z2,                  \n",
    "             \"A2\": A2}    \n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    print(\"Y.shape=\",Y.shape)\n",
    "    print(\"A2.shape=\",A2.shape)\n",
    "    #m = Y.shape[1] # number of example\n",
    "    m = Y.shape[0] # number of example\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2), 1-Y)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.\n",
    "    assert(isinstance(cost, float))    \n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']    \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1, 2))\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,                      \n",
    "             \"dW2\": dW2,             \n",
    "             \"db2\": db2}   \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']    \n",
    "    # Update rule for each parameter\n",
    "    W1 -= dW1 * learning_rate\n",
    "    b1 -= db1 * learning_rate\n",
    "    W2 -= dW2 * learning_rate\n",
    "    b2 -= db2 * learning_rate\n",
    "    parameters = {\"W1\": W1, \n",
    "                  \"b1\": b1,            \n",
    "                  \"W2\": W2,   \n",
    "                  \"b2\": b2}    \n",
    "    return parameters\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    w_x = layer_sizes(X, Y)[3]\n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y,w_x)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):        \n",
    "    # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)        \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)        \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=1.2)        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:            \n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))    \n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n",
      "X_train.shape= (309, 10) Y_train.shape= (309,) ;X_test.shape= (133, 10) Y_test.shape= (133,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "ylen = diabetes.target.shape\n",
    "Y = np.zeros(ylen)\n",
    "Y[diabetes.target>=np.mean(diabetes.target)]=1\n",
    "print(X.shape,Y.shape)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#x为数据集的feature熟悉，y为label.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "print(\"X_train.shape=\",X_train.shape,\"Y_train.shape=\",Y_train.shape,\";X_test.shape=\",X_test.shape,\"Y_test.shape=\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2.shape= (309, 10)  X.shape= (309, 10)\n",
      "Y.shape= (309,)\n",
      "A2.shape= (309, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (309,10) (309,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9217c476f485>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-ff7f54cd3572>\u001b[0m in \u001b[0;36mnn_model\u001b[1;34m(X, Y, n_h, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;31m# Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ff7f54cd3572>\u001b[0m in \u001b[0;36mcompute_cost\u001b[1;34m(A2, Y, parameters)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# number of example\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# Compute the cross-entropy cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mlogprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# makes sure cost is the dimension we expect.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (309,10) (309,) "
     ]
    }
   ],
   "source": [
    "d = nn_model(X_train, Y_train, 4, num_iterations = 10000,  print_cost = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
